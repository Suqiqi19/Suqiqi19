<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.2">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"suqiqi19.gitee.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="MADDPG 算法算法介绍简介多智能体 DDPG（muli-agent DDPG，MADDPG）是将 DDPG 算法扩展到多智能体环境中，从字面意思上来看就是对于每个智能体实现一个 DDPG 的算法。所有智能体共享一个中心化的 Critic 网络，该 Critic 网络在训练的过程中同时对每个智能体的 Actor 网络给出指导，而执行时每个智能体的 Actor 网络则是完全独立做出行动，即去中心化">
<meta property="og:type" content="article">
<meta property="og:title" content="MADDPG">
<meta property="og:url" content="https://suqiqi19.gitee.io/2023/03/13/MADDPG/index.html">
<meta property="og:site_name" content="慎独">
<meta property="og:description" content="MADDPG 算法算法介绍简介多智能体 DDPG（muli-agent DDPG，MADDPG）是将 DDPG 算法扩展到多智能体环境中，从字面意思上来看就是对于每个智能体实现一个 DDPG 的算法。所有智能体共享一个中心化的 Critic 网络，该 Critic 网络在训练的过程中同时对每个智能体的 Actor 网络给出指导，而执行时每个智能体的 Actor 网络则是完全独立做出行动，即去中心化">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://suqiqi19.gitee.io/2023/03/13/MADDPG/总览图.png">
<meta property="og:image" content="https://suqiqi19.gitee.io/2023/03/13/MADDPG/伪代码.png">
<meta property="article:published_time" content="2023-03-13T03:02:04.000Z">
<meta property="article:modified_time" content="2023-03-13T07:55:16.985Z">
<meta property="article:author" content="宿某人">
<meta property="article:tag" content="多智能体强化学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://suqiqi19.gitee.io/2023/03/13/MADDPG/总览图.png">

<link rel="canonical" href="https://suqiqi19.gitee.io/2023/03/13/MADDPG/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>MADDPG | 慎独</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">慎独</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">个人博客</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://suqiqi19.gitee.io/2023/03/13/MADDPG/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="宿某人">
      <meta itemprop="description" content="生命以负熵为生">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="慎独">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          MADDPG
        </h1>

        <div class="post-meta">
			
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-03-13 11:02:04 / 修改时间：15:55:16" itemprop="dateCreated datePublished" datetime="2023-03-13T11:02:04+08:00">2023-03-13</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/MARL/" itemprop="url" rel="index"><span itemprop="name">MARL</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>9.5k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>9 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="MADDPG-算法"><a href="#MADDPG-算法" class="headerlink" title="MADDPG 算法"></a>MADDPG 算法</h1><h2 id="算法介绍"><a href="#算法介绍" class="headerlink" title="算法介绍"></a>算法介绍</h2><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>多智能体 DDPG（muli-agent DDPG，MADDPG）是将 DDPG 算法扩展到多智能体环境中，从字面意思上来看就是对于每个智能体实现一个 DDPG 的算法。所有智能体共享一个中心化的 Critic 网络，该 Critic 网络在训练的过程中同时对每个智能体的 Actor 网络给出指导，而执行时每个智能体的 Actor 网络则是完全独立做出行动，即去中心化地执行。</p>
<span id="more"></span>
<h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>算法模型由多个DDPG网络组成，每个网络学习policy π (Actor) 和 action value Q (Critic)；同时具有target network，用于Q-learning的off-policy学习。算法架构图如下：</p>
<p><img src="/2023/03/13/MADDPG/总览图.png" alt="总览图"></p>
<p>每个智能体用 Actor-Critic 的方法训练，在 MADDPG 中每个智能体的 Critic 部分都能够获得其他智能体的策略信息。具体来说，考虑一个有$N$个智能体的博弈，每个智能体的策略参数为$\theta = \left \{  \theta_{1},…\theta_{N}  \right \}$ ,记$\pi = \left \{ \pi_{1},……,\pi_{N} \right \} $ 为所有智能体的策略集合，那么我们可以写出在随机性策略情况下每个智能体的累积期望奖励$J\left(\theta_{i}\right)=E_{s \sim \rho^{\pi}, a_{i} \sim \pi_{\theta_{i}}}\left[\sum_{t=0}^{\infty} \gamma^{t} r_{i, t}\right]$ ,针对随机策略，求策略梯度为：</p>
<script type="math/tex; mode=display">
J\left(\theta_{i}\right)=E_{s \sim \rho^{\pi}, a_{i} \sim \pi_{\theta_{i}}}\left[\sum_{t=0}^{\infty} \gamma^{t} r_{i, t}\right]\nabla_{\theta_{i}} J\left(\theta_{i}\right)=\mathbb{E}_{s \sim p^{\mu}, a \sim \pi_{i}}\left[\nabla_{\theta_{i}} \log \pi_{i}\left(a_{i} \mid o_{i}\right) Q_{i}^{\pi}\left(\mathbf{x}, a_{1}, \ldots, a_{N}\right)\right]</script><p>其中，$Q_{i}^{\pi}\left(\mathbf{x}, a_{1}, \ldots, a_{N}\right)$就是一个中心化的动作价值函数。为什么说$Q_{i}$是一个中心化的动作价值函数呢？一般来说$x=(o_{1},…,o_{N})$包含了所有智能体的观测，另外$Q_{i}$也需要输入所有智能体在此刻的动作，因此$Q_{i}$工作的前提就是所有智能体要同时给出自己的观测和相应的动作。</p>
<p>对于确定性策略来说，考虑现在有N个连续的策略$\mu _{\theta _{i}}$，可以得到 DDPG 的梯度公式：</p>
<script type="math/tex; mode=display">
\nabla_{\theta_i} J\left(\mu_i\right)=\mathbb{E}_{\mathbf{x} \sim \mathcal{D}}\left[\left.\nabla_{\theta_i} \mu_i\left(o_i\right) \nabla_{a_i} Q_i^\mu\left(\mathbf{x}, a_1, \ldots, a_N\right)\right|_{a_i=\mu_i\left(o_i\right)}\right]</script><p>其中，$D$是我们用来存储数据的经验回放池，它存储的每一个数据为$\left(\mathbf{x}, \mathbf{x}^{\prime}, a_1, \ldots, a_N, r_1, \ldots, r_N\right)$。而在 MADDPG 中，中心化动作价值函数可以按照下面的损失函数来更新：</p>
<script type="math/tex; mode=display">
\mathcal{L}\left(\omega_i\right)=\mathbb{E}_{\mathbf{x}, a, r, \mathbf{x}^{\prime}}\left[\left(Q_i^\mu\left(\mathbf{x}, a_1, \ldots, a_N\right)-y\right)^2\right], \quad y=r_i+\left.\gamma Q_i^{\mu^{\prime}}\left(\mathbf{x}^{\prime}, a_1^{\prime}, \ldots, a_N^{\prime}\right)\right|_{a_j^{\prime}=\mu_j^{\prime}\left(o_j\right)}</script><p>流程为：</p>
<p><img src="/2023/03/13/MADDPG/伪代码.png" alt="伪代码"></p>
<h3 id="算法实现"><a href="#算法实现" class="headerlink" title="算法实现"></a>算法实现</h3><h5 id="导入一些需要用到的包。"><a href="#导入一些需要用到的包。" class="headerlink" title="导入一些需要用到的包。"></a>导入一些需要用到的包。</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> rl_utils</span><br></pre></td></tr></table></figure>
<h5 id="创建环境"><a href="#创建环境" class="headerlink" title="创建环境"></a>创建环境</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">make_env</span>(<span class="params">scenario_name</span>):</span><br><span class="line">    <span class="comment"># 从环境文件脚本中创建环境</span></span><br><span class="line">    scenario = scenarios.load(scenario_name + <span class="string">&quot;.py&quot;</span>).Scenario()</span><br><span class="line">    world = scenario.make_world()</span><br><span class="line">    env = MultiAgentEnv(world, scenario.reset_world, scenario.reward,</span><br><span class="line">                        scenario.observation)</span><br><span class="line">    <span class="keyword">return</span> env</span><br></pre></td></tr></table></figure>
<h5 id="Gumbel-Softmax-采样"><a href="#Gumbel-Softmax-采样" class="headerlink" title="Gumbel-Softmax 采样"></a>Gumbel-Softmax 采样</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">onehot_from_logits</span>(<span class="params">logits, eps=<span class="number">0.01</span></span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27; 生成最优动作的独热（one-hot）形式 &#x27;&#x27;&#x27;</span></span><br><span class="line">    argmax_acs = (logits == logits.<span class="built_in">max</span>(<span class="number">1</span>, keepdim=<span class="literal">True</span>)[<span class="number">0</span>]).<span class="built_in">float</span>()</span><br><span class="line">    <span class="comment"># 生成随机动作,转换成独热形式</span></span><br><span class="line">    rand_acs = torch.autograd.Variable(torch.eye(logits.shape[<span class="number">1</span>])[[</span><br><span class="line">        np.random.choice(<span class="built_in">range</span>(logits.shape[<span class="number">1</span>]), size=logits.shape[<span class="number">0</span>])</span><br><span class="line">    ]],</span><br><span class="line">                                       requires_grad=<span class="literal">False</span>).to(logits.device)</span><br><span class="line">    <span class="comment"># 通过epsilon-贪婪算法来选择用哪个动作</span></span><br><span class="line">    <span class="keyword">return</span> torch.stack([</span><br><span class="line">        argmax_acs[i] <span class="keyword">if</span> r &gt; eps <span class="keyword">else</span> rand_acs[i]</span><br><span class="line">        <span class="keyword">for</span> i, r <span class="keyword">in</span> <span class="built_in">enumerate</span>(torch.rand(logits.shape[<span class="number">0</span>]))</span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sample_gumbel</span>(<span class="params">shape, eps=<span class="number">1e-20</span>, tens_type=torch.FloatTensor</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;从Gumbel(0,1)分布中采样&quot;&quot;&quot;</span></span><br><span class="line">    U = torch.autograd.Variable(tens_type(*shape).uniform_(),</span><br><span class="line">                                requires_grad=<span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">return</span> -torch.log(-torch.log(U + eps) + eps)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gumbel_softmax_sample</span>(<span class="params">logits, temperature</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; 从Gumbel-Softmax分布中采样&quot;&quot;&quot;</span></span><br><span class="line">    y = logits + sample_gumbel(logits.shape, tens_type=<span class="built_in">type</span>(logits.data)).to(</span><br><span class="line">        logits.device)</span><br><span class="line">    <span class="keyword">return</span> F.softmax(y / temperature, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gumbel_softmax</span>(<span class="params">logits, temperature=<span class="number">1.0</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;从Gumbel-Softmax分布中采样,并进行离散化&quot;&quot;&quot;</span></span><br><span class="line">    y = gumbel_softmax_sample(logits, temperature)</span><br><span class="line">    y_hard = onehot_from_logits(y)</span><br><span class="line">    y = (y_hard.to(logits.device) - y).detach() + y</span><br><span class="line">    <span class="comment"># 返回一个y_hard的独热量,但是它的梯度是y,我们既能够得到一个与环境交互的离散动作,又可以</span></span><br><span class="line">    <span class="comment"># 正确地反传梯度</span></span><br><span class="line">    <span class="keyword">return</span> y</span><br></pre></td></tr></table></figure>
<h5 id="单智能体-DDPG"><a href="#单智能体-DDPG" class="headerlink" title="单智能体 DDPG"></a>单智能体 DDPG</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TwoLayerFC</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_in, num_out, hidden_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.fc1 = torch.nn.Linear(num_in, hidden_dim)</span><br><span class="line">        self.fc2 = torch.nn.Linear(hidden_dim, hidden_dim)</span><br><span class="line">        self.fc3 = torch.nn.Linear(hidden_dim, num_out)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        <span class="keyword">return</span> self.fc3(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DDPG</span>:</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27; DDPG算法 &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, state_dim, action_dim, critic_input_dim, hidden_dim,</span></span><br><span class="line"><span class="params">                 actor_lr, critic_lr, device</span>):</span><br><span class="line">        self.actor = TwoLayerFC(state_dim, action_dim, hidden_dim).to(device)</span><br><span class="line">        self.target_actor = TwoLayerFC(state_dim, action_dim,</span><br><span class="line">                                       hidden_dim).to(device)</span><br><span class="line">        self.critic = TwoLayerFC(critic_input_dim, <span class="number">1</span>, hidden_dim).to(device)</span><br><span class="line">        self.target_critic = TwoLayerFC(critic_input_dim, <span class="number">1</span>,</span><br><span class="line">                                        hidden_dim).to(device)</span><br><span class="line">        self.target_critic.load_state_dict(self.critic.state_dict())</span><br><span class="line">        self.target_actor.load_state_dict(self.actor.state_dict())</span><br><span class="line">        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(),</span><br><span class="line">                                                lr=actor_lr)</span><br><span class="line">        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(),</span><br><span class="line">                                                 lr=critic_lr)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">take_action</span>(<span class="params">self, state, explore=<span class="literal">False</span></span>):</span><br><span class="line">        action = self.actor(state)</span><br><span class="line">        <span class="keyword">if</span> explore:</span><br><span class="line">            action = gumbel_softmax(action)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            action = onehot_from_logits(action)</span><br><span class="line">        <span class="keyword">return</span> action.detach().cpu().numpy()[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">soft_update</span>(<span class="params">self, net, target_net, tau</span>):</span><br><span class="line">        <span class="keyword">for</span> param_target, param <span class="keyword">in</span> <span class="built_in">zip</span>(target_net.parameters(),</span><br><span class="line">                                       net.parameters()):</span><br><span class="line">            param_target.data.copy_(param_target.data * (<span class="number">1.0</span> - tau) +</span><br><span class="line">                                    param.data * tau)</span><br></pre></td></tr></table></figure>
<h5 id="MADDPG-类"><a href="#MADDPG-类" class="headerlink" title="MADDPG 类"></a>MADDPG 类</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MADDPG</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, env, device, actor_lr, critic_lr, hidden_dim,</span></span><br><span class="line"><span class="params">                 state_dims, action_dims, critic_input_dim, gamma, tau</span>):</span><br><span class="line">        self.agents = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(env.agents)):</span><br><span class="line">            self.agents.append(</span><br><span class="line">                DDPG(state_dims[i], action_dims[i], critic_input_dim,</span><br><span class="line">                     hidden_dim, actor_lr, critic_lr, device))</span><br><span class="line">        self.gamma = gamma</span><br><span class="line">        self.tau = tau</span><br><span class="line">        self.critic_criterion = torch.nn.MSELoss()</span><br><span class="line">        self.device = device</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">policies</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> [agt.actor <span class="keyword">for</span> agt <span class="keyword">in</span> self.agents]</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">target_policies</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> [agt.target_actor <span class="keyword">for</span> agt <span class="keyword">in</span> self.agents]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">take_action</span>(<span class="params">self, states, explore</span>):</span><br><span class="line">        states = [</span><br><span class="line">            torch.tensor([states[i]], dtype=torch.<span class="built_in">float</span>, device=self.device)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(env.agents))</span><br><span class="line">        ]</span><br><span class="line">        <span class="keyword">return</span> [</span><br><span class="line">            agent.take_action(state, explore)</span><br><span class="line">            <span class="keyword">for</span> agent, state <span class="keyword">in</span> <span class="built_in">zip</span>(self.agents, states)</span><br><span class="line">        ]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">update</span>(<span class="params">self, sample, i_agent</span>):</span><br><span class="line">        obs, act, rew, next_obs, done = sample</span><br><span class="line">        cur_agent = self.agents[i_agent]</span><br><span class="line"></span><br><span class="line">        cur_agent.critic_optimizer.zero_grad()</span><br><span class="line">        all_target_act = [</span><br><span class="line">            onehot_from_logits(pi(_next_obs))</span><br><span class="line">            <span class="keyword">for</span> pi, _next_obs <span class="keyword">in</span> <span class="built_in">zip</span>(self.target_policies, next_obs)</span><br><span class="line">        ]</span><br><span class="line">        target_critic_input = torch.cat((*next_obs, *all_target_act), dim=<span class="number">1</span>)</span><br><span class="line">        target_critic_value = rew[i_agent].view(</span><br><span class="line">            -<span class="number">1</span>, <span class="number">1</span>) + self.gamma * cur_agent.target_critic(</span><br><span class="line">                target_critic_input) * (<span class="number">1</span> - done[i_agent].view(-<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        critic_input = torch.cat((*obs, *act), dim=<span class="number">1</span>)</span><br><span class="line">        critic_value = cur_agent.critic(critic_input)</span><br><span class="line">        critic_loss = self.critic_criterion(critic_value,</span><br><span class="line">                                            target_critic_value.detach())</span><br><span class="line">        critic_loss.backward()</span><br><span class="line">        cur_agent.critic_optimizer.step()</span><br><span class="line"></span><br><span class="line">        cur_agent.actor_optimizer.zero_grad()</span><br><span class="line">        cur_actor_out = cur_agent.actor(obs[i_agent])</span><br><span class="line">        cur_act_vf_in = gumbel_softmax(cur_actor_out)</span><br><span class="line">        all_actor_acs = []</span><br><span class="line">        <span class="keyword">for</span> i, (pi, _obs) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(self.policies, obs)):</span><br><span class="line">            <span class="keyword">if</span> i == i_agent:</span><br><span class="line">                all_actor_acs.append(cur_act_vf_in)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                all_actor_acs.append(onehot_from_logits(pi(_obs)))</span><br><span class="line">        vf_in = torch.cat((*obs, *all_actor_acs), dim=<span class="number">1</span>)</span><br><span class="line">        actor_loss = -cur_agent.critic(vf_in).mean()</span><br><span class="line">        actor_loss += (cur_actor_out**<span class="number">2</span>).mean() * <span class="number">1e-3</span></span><br><span class="line">        actor_loss.backward()</span><br><span class="line">        cur_agent.actor_optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">update_all_targets</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">for</span> agt <span class="keyword">in</span> self.agents:</span><br><span class="line">            agt.soft_update(agt.actor, agt.target_actor, self.tau)</span><br><span class="line">            agt.soft_update(agt.critic, agt.target_critic, self.tau)</span><br></pre></td></tr></table></figure>
<h5 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">num_episodes = <span class="number">5000</span></span><br><span class="line">episode_length = <span class="number">25</span>  <span class="comment"># 每条序列的最大长度</span></span><br><span class="line">buffer_size = <span class="number">100000</span></span><br><span class="line">hidden_dim = <span class="number">64</span></span><br><span class="line">actor_lr = <span class="number">1e-2</span></span><br><span class="line">critic_lr = <span class="number">1e-2</span></span><br><span class="line">gamma = <span class="number">0.95</span></span><br><span class="line">tau = <span class="number">1e-2</span></span><br><span class="line">batch_size = <span class="number">1024</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">update_interval = <span class="number">100</span></span><br><span class="line">minimal_size = <span class="number">4000</span></span><br><span class="line"></span><br><span class="line">env_id = <span class="string">&quot;simple_adversary&quot;</span></span><br><span class="line">env = make_env(env_id)</span><br><span class="line">replay_buffer = rl_utils.ReplayBuffer(buffer_size)</span><br><span class="line"></span><br><span class="line">state_dims = []</span><br><span class="line">action_dims = []</span><br><span class="line"><span class="keyword">for</span> action_space <span class="keyword">in</span> env.action_space:</span><br><span class="line">    action_dims.append(action_space.n)</span><br><span class="line"><span class="keyword">for</span> state_space <span class="keyword">in</span> env.observation_space:</span><br><span class="line">    state_dims.append(state_space.shape[<span class="number">0</span>])</span><br><span class="line">critic_input_dim = <span class="built_in">sum</span>(state_dims) + <span class="built_in">sum</span>(action_dims)</span><br><span class="line"></span><br><span class="line">maddpg = MADDPG(env, device, actor_lr, critic_lr, hidden_dim, state_dims,</span><br><span class="line">                action_dims, critic_input_dim, gamma, tau)</span><br></pre></td></tr></table></figure>
<h5 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate</span>(<span class="params">env_id, maddpg, n_episode=<span class="number">10</span>, episode_length=<span class="number">25</span></span>):</span><br><span class="line">    <span class="comment"># 对学习的策略进行评估,此时不会进行探索</span></span><br><span class="line">    env = make_env(env_id)</span><br><span class="line">    returns = np.zeros(<span class="built_in">len</span>(env.agents))</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_episode):</span><br><span class="line">        obs = env.reset()</span><br><span class="line">        <span class="keyword">for</span> t_i <span class="keyword">in</span> <span class="built_in">range</span>(episode_length):</span><br><span class="line">            actions = maddpg.take_action(obs, explore=<span class="literal">False</span>)</span><br><span class="line">            obs, rew, done, info = env.step(actions)</span><br><span class="line">            rew = np.array(rew)</span><br><span class="line">            returns += rew / n_episode</span><br><span class="line">    <span class="keyword">return</span> returns.tolist()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">return_list = []  <span class="comment"># 记录每一轮的回报（return）</span></span><br><span class="line">total_step = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i_episode <span class="keyword">in</span> <span class="built_in">range</span>(num_episodes):</span><br><span class="line">    state = env.reset()</span><br><span class="line">    <span class="comment"># ep_returns = np.zeros(len(env.agents))</span></span><br><span class="line">    <span class="keyword">for</span> e_i <span class="keyword">in</span> <span class="built_in">range</span>(episode_length):</span><br><span class="line">        actions = maddpg.take_action(state, explore=<span class="literal">True</span>)</span><br><span class="line">        next_state, reward, done, _ = env.step(actions)</span><br><span class="line">        replay_buffer.add(state, actions, reward, next_state, done)</span><br><span class="line">        state = next_state</span><br><span class="line"></span><br><span class="line">        total_step += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> replay_buffer.size(</span><br><span class="line">        ) &gt;= minimal_size <span class="keyword">and</span> total_step % update_interval == <span class="number">0</span>:</span><br><span class="line">            sample = replay_buffer.sample(batch_size)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">def</span> <span class="title function_">stack_array</span>(<span class="params">x</span>):</span><br><span class="line">                rearranged = [[sub_x[i] <span class="keyword">for</span> sub_x <span class="keyword">in</span> x]</span><br><span class="line">                              <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(x[<span class="number">0</span>]))]</span><br><span class="line">                <span class="keyword">return</span> [</span><br><span class="line">                    torch.FloatTensor(np.vstack(aa)).to(device)</span><br><span class="line">                    <span class="keyword">for</span> aa <span class="keyword">in</span> rearranged</span><br><span class="line">                ]</span><br><span class="line"></span><br><span class="line">            sample = [stack_array(x) <span class="keyword">for</span> x <span class="keyword">in</span> sample]</span><br><span class="line">            <span class="keyword">for</span> a_i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(env.agents)):</span><br><span class="line">                maddpg.update(sample, a_i)</span><br><span class="line">            maddpg.update_all_targets()</span><br><span class="line">    <span class="keyword">if</span> (i_episode + <span class="number">1</span>) % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">        ep_returns = evaluate(env_id, maddpg, n_episode=<span class="number">100</span>)</span><br><span class="line">        return_list.append(ep_returns)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Episode: <span class="subst">&#123;i_episode+<span class="number">1</span>&#125;</span>, <span class="subst">&#123;ep_returns&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<h5 id="效果"><a href="#效果" class="headerlink" title="效果"></a>效果</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">return_array = np.array(return_list)</span><br><span class="line"><span class="keyword">for</span> i, agent_name <span class="keyword">in</span> <span class="built_in">enumerate</span>([<span class="string">&quot;adversary_0&quot;</span>, <span class="string">&quot;agent_0&quot;</span>, <span class="string">&quot;agent_1&quot;</span>]):</span><br><span class="line">    plt.figure()</span><br><span class="line">    plt.plot(</span><br><span class="line">        np.arange(return_array.shape[<span class="number">0</span>]) * <span class="number">100</span>,</span><br><span class="line">        rl_utils.moving_average(return_array[:, i], <span class="number">9</span>))</span><br><span class="line">    plt.xlabel(<span class="string">&quot;Episodes&quot;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&quot;Returns&quot;</span>)</span><br><span class="line">    plt.title(<span class="string">f&quot;<span class="subst">&#123;agent_name&#125;</span> by MADDPG&quot;</span>)</span><br></pre></td></tr></table></figure>

    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>宿某人
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://suqiqi19.gitee.io/2023/03/13/MADDPG/" title="MADDPG">https://suqiqi19.gitee.io/2023/03/13/MADDPG/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" rel="tag"># 多智能体强化学习</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2023/03/13/%E6%8E%A2%E7%B4%A2%E4%B8%8E%E5%88%A9%E7%94%A8/" rel="prev" title="探索与利用">
      <i class="fa fa-chevron-left"></i> 探索与利用
    </a></div>
      <div class="post-nav-item">
    <a href="/2023/03/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%AF%B9%E6%8A%97/" rel="next" title="多智能体对抗">
      多智能体对抗 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#MADDPG-%E7%AE%97%E6%B3%95"><span class="nav-number">1.</span> <span class="nav-text">MADDPG 算法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AE%97%E6%B3%95%E4%BB%8B%E7%BB%8D"><span class="nav-number">1.1.</span> <span class="nav-text">算法介绍</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AE%80%E4%BB%8B"><span class="nav-number">1.1.1.</span> <span class="nav-text">简介</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8E%9F%E7%90%86"><span class="nav-number">1.1.2.</span> <span class="nav-text">原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0"><span class="nav-number">1.1.3.</span> <span class="nav-text">算法实现</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%AF%BC%E5%85%A5%E4%B8%80%E4%BA%9B%E9%9C%80%E8%A6%81%E7%94%A8%E5%88%B0%E7%9A%84%E5%8C%85%E3%80%82"><span class="nav-number">1.1.3.0.1.</span> <span class="nav-text">导入一些需要用到的包。</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA%E7%8E%AF%E5%A2%83"><span class="nav-number">1.1.3.0.2.</span> <span class="nav-text">创建环境</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Gumbel-Softmax-%E9%87%87%E6%A0%B7"><span class="nav-number">1.1.3.0.3.</span> <span class="nav-text">Gumbel-Softmax 采样</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%8D%95%E6%99%BA%E8%83%BD%E4%BD%93-DDPG"><span class="nav-number">1.1.3.0.4.</span> <span class="nav-text">单智能体 DDPG</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#MADDPG-%E7%B1%BB"><span class="nav-number">1.1.3.0.5.</span> <span class="nav-text">MADDPG 类</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%8F%82%E6%95%B0"><span class="nav-number">1.1.3.0.6.</span> <span class="nav-text">参数</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83"><span class="nav-number">1.1.3.0.7.</span> <span class="nav-text">训练</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%95%88%E6%9E%9C"><span class="nav-number">1.1.3.0.8.</span> <span class="nav-text">效果</span></a></li></ol></li></ol></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="宿某人"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">宿某人</p>
  <div class="site-description" itemprop="description">生命以负熵为生</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">8</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="/1323124280@qq.com" title="E-Mail → 1323124280@qq.com"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>



      </div>
		    
    <script type="text/javascript" charset="utf-8" src="/js/tagcloud.js"></script>
    <script type="text/javascript" charset="utf-8" src="/js/tagcanvas.js"></script>
    <div class="widget-wrap">
        <h3 class="widget-title">Tag Cloud</h3>
        <div id="myCanvasContainer" class="widget tagcloud">
            <canvas width="250" height="250" id="resCanvas" style="width=100%">
                <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/F0601/" rel="tag">F0601</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/gitee/" rel="tag">gitee</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hexo/" rel="tag">hexo</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" rel="tag">多智能体强化学习</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" rel="tag">强化学习</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%AF%95%E8%AE%BE/" rel="tag">毕设</a><span class="tag-list-count">1</span></li></ul>
            </canvas>
        </div>
    </div>
    

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">宿某人</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="站点总字数">18k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">16 分钟</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  
  <script color='0,0,0' opacity='0.5' zIndex='-1' count='150' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script size="300" alpha="0.6" zIndex="-1" src="/lib/canvas-ribbon/canvas-ribbon.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>

<script src="/js/bookmark.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

      <!-- 页面点击小红心 -->
    <script type="text/javascript" src="/js/clicklove.js"></script>

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"model":{"jsonPath":"/live2dw/assets/wanko.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":false},"log":false});</script></body>
</html>
